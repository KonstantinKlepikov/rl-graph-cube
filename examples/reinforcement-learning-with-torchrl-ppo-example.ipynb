{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Reinforcement Learning with TorchRL (PPO example)\n","\n","Main reason for me joining this competition is to further explore RL, even if it's not the best solution for the problem. In this notebook, I'll attempt to make the RL version of the problem approachable. I've opted to use TorchRL library, hoping to gain access to some common RL infra, such as replay buffers and implementations of most common algorithms.\n","\n","Unfortunately, I haven't been able to install torchrl in kaggle environment, so I'm uploading notebook that was run on another machine.\n","\n","I'm assuming there is an existing environment with PyTorch already installed."]},{"cell_type":"markdown","metadata":{},"source":["## Utils for competition data"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from ast import literal_eval\n","import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from pprint import pprint\n","from sympy.combinatorics import Permutation\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["DATA_DIR = Path(\"/kaggle/input/santa-2023\")\n","PUZZLES = DATA_DIR / \"puzzles.csv\"\n","PUZZLES_INFO = DATA_DIR / \"puzzle_info.csv\"\n","SAMPLE_SUBMISSION = DATA_DIR / \"sample_submission.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_puzzles_info():\n","    puzzle_info = pd.read_csv(PUZZLES_INFO, index_col=\"puzzle_type\")\n","    puzzle_info[\"allowed_moves\"] = puzzle_info[\"allowed_moves\"].apply(literal_eval)\n","    return puzzle_info\n","\n","\n","def load_puzzles():\n","    puzzles = pd.read_csv(PUZZLES, index_col=\"id\")\n","    puzzles[\"id\"] = puzzles.index\n","    # Parse color states\n","    puzzles = puzzles.assign(\n","        initial_state=lambda df: df[\"initial_state\"].str.split(\";\"),\n","        solution_state=lambda df: df[\"solution_state\"].str.split(\";\"),\n","    )\n","    puzzles.loc[:, \"solution_state\"] = puzzles[\"solution_state\"].apply(\n","        lambda x: tuple(x)\n","    )\n","    puzzles.loc[:, \"initial_state\"] = puzzles[\"initial_state\"].apply(lambda x: tuple(x))\n","    return puzzles\n","\n","\n","def load_sample_submission():\n","    sample_submission = pd.read_csv(SAMPLE_SUBMISSION)\n","    # sample_submission[\"id\"] = sample_submission.index\n","    return sample_submission\n","\n","\n","def get_puzzles(\n","    puzzle_type=\"cube_2/2/2\",\n","    puzzles=None,\n","    puzzle_info=None,\n","    sample_submission=None,\n","):\n","    puzzles = puzzles or load_puzzles()\n","    puzzle_info = puzzle_info or load_puzzles_info()\n","    sample_submission = sample_submission or load_sample_submission()\n","    if puzzle_type:\n","        puzzle_info = puzzle_info.loc[puzzle_type]\n","        puzzles = puzzles.query(f\"puzzle_type == '{puzzle_type}'\")\n","        sample_submission = sample_submission.loc[puzzles.index]\n","    return puzzle_info.allowed_moves, puzzles, sample_submission"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["moves, puzzles, sample_sub = get_puzzles(\"cube_2/2/2\")"]},{"cell_type":"markdown","metadata":{},"source":["## RL environment\n","\n","TorchRL has a concept of environments, similar to gymnasium (ex OpenAI Gym). To implement an environment, we extend from `EnvBase`. We have to define **`observation_spec`**, **`action_spec`** and **`reward_spec`** which describe the shape of tensors and kind of data these tensors hold. We also need to extend methods **`_reset`**, **`_step`** and \n","**`_set_seed`**.\n","\n","TorchRL dosn't use pure tensors for input/output, but introduces  **TensorDicts**. TensorDicts act like dicts of tensors, but they all share batch size, and many common operations can be taken on a tensordict and will affect all tensors within.\n","\n","For the santa challenge, we will have independent instances of environment for every puzzle type. Note that for every `puzzle_type` in the data, there are several different puzzle complexities, for example with different number of individual colors. We treat those as different environments here.\n","\n","Note that it is also possible to implement the environment in gymnasium, and apply a TorchRL wrapper `GymWrapper` for it to be available in TorchRL. Here we explore the TorchRL-native route."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Optional\n","import torch\n","from torchrl.envs import EnvBase\n","from tensordict import TensorDict\n","from torchrl.data import (\n","    OneHotDiscreteTensorSpec,\n","    DiscreteTensorSpec,\n","    CompositeSpec,\n","    UnboundedContinuousTensorSpec,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Hardcoded initial and solution states, for simpliciy\n","INITIAL_STATE_COLORS = (\n","    \"D\", \"E\", \"D\", \"A\",\n","    \"E\", \"B\", \"A\", \"B\",\n","    \"C\", \"A\", \"C\", \"A\",\n","    \"D\", \"C\", \"D\", \"F\",\n","    \"F\", \"F\", \"E\", \"E\",\n","    \"B\", \"F\", \"B\", \"C\",\n",")\n","SOLUTION_STATE_COLORS = (\n","    \"A\", \"A\", \"A\", \"A\",\n","    \"B\", \"B\", \"B\", \"B\",\n","    \"C\", \"C\", \"C\", \"C\",\n","    \"D\", \"D\", \"D\", \"D\",\n","    \"E\", \"E\", \"E\", \"E\",\n","    \"F\", \"F\", \"F\", \"F\",\n",")\n","\n","class PuzzleEnv(EnvBase):\n","    def __init__(\n","            self,\n","            state_size: int,\n","            colors: list,\n","            moves: dict,\n","            solution_state: list = SOLUTION_STATE_COLORS,\n","            **kwargs\n","    ):\n","        \"\"\"\n","        Args:\n","            state_size: number of \"squares\" in the puzzle\n","            colors: all possible colors in this problem\n","            moves: moves dict from `puzzle_info.csv` for this puzzle type\n","            solution_state: final solution state\n","        \"\"\"\n","        super().__init__(**kwargs)\n","        self.state_size = state_size\n","        # Consider all forward and backward moves\n","        self.move_names = list(moves.keys()) + [f\"-{name}\" for name in moves.keys()]\n","        moves_fwd = torch.tensor(list(moves.values()))\n","        moves_bwd = torch.argsort(moves_fwd)\n","        self.moves = torch.vstack((moves_fwd, moves_bwd))\n","\n","        self.colors = colors\n","        self.colormap = {c: i for i, c in enumerate(colors)}\n","        self.n_colors = len(colors)\n","\n","        self.solution_state = solution_state\n","\n","        # TorchRL specs - needs to be initialized\n","        # actions from puzzle_info\n","        self.action_spec = OneHotDiscreteTensorSpec(\n","            2 * len(moves), shape=torch.Size([2 * len(moves)]), dtype=torch.int32\n","        )\n","        # Note that observation spec needs to be CompositeSpec.\n","        observation_spec = DiscreteTensorSpec(\n","            n=self.n_colors,\n","            shape=torch.Size([self.state_size]),\n","            dtype=torch.float,\n","        )\n","        self.observation_spec = CompositeSpec(observation=observation_spec, shape=None)\n","        # Rewards.\n","        self.reward_spec = UnboundedContinuousTensorSpec(shape=torch.Size([1]))\n","\n","    def _reset(self, tensordict: Optional[TensorDict], **kwargs):\n","        \"\"\"Basic implementation, sets hardcoded INITIAL_STATE.\n","        Also sets \"num_wildcards\", which we carry as the unofficial part of the state.\n","        \"\"\"\n","        if tensordict is not None:\n","            return tensordict.clone()\n","        # initial_state = [self.colormap[color] for color in INITIAL_STATE_COLORS]\n","        initial_state = self._state_to_tensor(INITIAL_STATE_COLORS)\n","        out_tensordict = TensorDict({}, batch_size=torch.Size())\n","        self.state = initial_state\n","        out_tensordict.set(\"observation\", self.state)\n","        out_tensordict.set(\"num_wildcards\", torch.tensor(0))\n","        return out_tensordict\n","\n","    def _step(self, tensordict):\n","        \"\"\"\n","        Args\n","            tensordict: We expet fields \"action\" according to action_spec.\n","            We also expect \"num_wildcards\" here.\n","\n","        \"\"\"\n","        solution_state = self._state_to_tensor(self.solution_state)\n","        num_wildcards = tensordict[\"num_wildcards\"]\n","        actions_onehot = tensordict[\"action\"]\n","        actions_idx = actions_onehot.argmax(dim=-1).tolist()\n","        # Permutations to apply, one row per batch\n","        moves = self.moves[actions_idx]\n","        self.state = torch.gather(self.state, -1, moves)\n","        errors = torch.sum(~(self.state == solution_state), dim=-1)\n","        final = ~(torch.relu(errors - num_wildcards).bool())\n","        # rewards are 0 in final state, -1 otherwise\n","        rewards = final * 1.0 - 1.0\n","        out_tensordict = TensorDict(\n","            {\n","                \"observation\": self.state,\n","                \"reward\": rewards,\n","                \"done\": final,\n","            },\n","            batch_size=self.batch_size,\n","        )\n","        return out_tensordict\n","\n","    def _set_seed(self, seed):\n","        \"\"\"No need to do stuff here.\n","\n","        super().seed() already sets the torch seed, and then calls this.\n","        \"\"\"\n","        pass\n","\n","    def _state_to_tensor(self, state):\n","        return torch.tensor([self.colormap[color] for color in state]).float()\n"]},{"cell_type":"markdown","metadata":{},"source":["Let's create an instance of our environment."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env = PuzzleEnv(\n","    # cube_2/2/2 has 24 \"squares\"\n","    state_size=24,\n","    colors=[\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"],\n","    moves=moves,\n",")"]},{"cell_type":"markdown","metadata":{},"source":["We can use `fake_tensordict()` to create a tensordict with all zeros, but valid according to all specs for this environment.\n","\n","This is a good sanity check that our specs are OK."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env.fake_tensordict()"]},{"cell_type":"markdown","metadata":{},"source":["We can call `rollout` on the environment. If we don't provide a policy, it will use a random policy by default. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["env.rollout(3)"]},{"cell_type":"markdown","metadata":{},"source":["## Transforms\n","\n","We can add transforms to our environment, to make it easier for training, or add some additional info. In gymnasium, that is achieved with wrappers, but TorchRL takes approach similar to other PyTorch libraries and offers `TransformedEnv` class that takes environment and applies transforms to it.\n","\n","We can normalize observation inputs by applying `ObservationNorm` transform. Note that this transform will need to be initialized to work correctly, so we call `init_stats` which will run some iterations in the background for us.\n","\n","We can also apply casting operations, such as `DoubleToFloat` if our model expects tensors of different type that the environment offers.\n","\n","The StepCounter transform will be used to count the steps before the environment is terminated. We can use this measure as a supplementary measure of performance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchrl.envs import (\n","    Compose,\n","    DoubleToFloat,\n","    ObservationNorm,\n","    StepCounter,\n","    TransformedEnv,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformed_env = TransformedEnv(\n","    env,\n","    Compose(\n","        # normalize observations\n","        ObservationNorm(in_keys=[\"observation\"]),\n","        # Our observations are already float.\n","        #DoubleToFloat(\n","        #    in_keys=[\"observation\"],\n","        #),\n","        StepCounter(),\n","    ),\n",")\n","\n","# Initialize ObservationNorm\n","transformed_env.transform[0].init_stats(num_iter=1000, reduce_dim=0, cat_dim=0)"]},{"cell_type":"markdown","metadata":{},"source":["## Models\n","\n","PPO is an actor-critic algorithm so we need both the actor model that will directly output the policy action, and the critic  model that is used to guide the actor."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from tensordict.nn import TensorDictModule\n","from tensordict.nn.distributions import NormalParamExtractor\n","from torch import nn\n","from torchrl.modules import ProbabilisticActor, TanhNormal, ValueOperator"]},{"cell_type":"markdown","metadata":{},"source":["Here we'll add the actor model and TorchRL's `ProbabilisticActor`, which can be used as a policy within environments. `ProbabilisticActor` requires distribution of inputs, so for every action, our model will need to output a probability distribution of potential values. This can be achieved by the model outputing `2 * action_spec.shape`, followed by a `NormalParamExtractor` module. `NormalParamExtractor` will output a tensordict with keys \"loc\" and \"scale\", which are expected by `ProbabilisticActor`.\n","\n","Note that our `nn.Module` is further wrapped into `TensorDictModule`, which has information about expected `in_keys` and `out_keys`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["actor_net = nn.Sequential(\n","    nn.Linear(24, 128), nn.Tanh(),\n","    nn.Linear(128, 128), nn.Tanh(),\n","    nn.Linear(128, 128), nn.Tanh(),\n","    nn.LazyLinear(2 * env.action_spec.shape[-1]),\n","    NormalParamExtractor(),  # extract \"loc\" and \"scale\"\n",")\n","policy_module = TensorDictModule(\n","    actor_net, in_keys=[\"observation\"], out_keys=[\"loc\", \"scale\"]\n",")\n","\n","actor = ProbabilisticActor(\n","    policy_module,\n","    spec=env.action_spec,\n","    in_keys=[\"loc\", \"scale\"],\n","    distribution_class=TanhNormal,\n","    distribution_kwargs={\"min\": 0.0, \"max\": 1.0},\n","    return_log_prob=True\n",")"]},{"cell_type":"markdown","metadata":{},"source":["Similarly for the Critic model, we will use TorchRL's `ValueOperator` which will output a scalar, our prediction for reward."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["critic = ValueOperator(\n","    nn.Sequential(\n","        # alternatively use nn.LazyLinear\n","        nn.Linear(24, 128), nn.Tanh(),\n","        nn.Linear(128, 128), nn.Tanh(),\n","        nn.Linear(128, 128), nn.Tanh(),\n","        nn.Linear(128, 1),\n","    ),\n","    in_keys=[\"observation\"],\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Data collector\n","\n","TorchRL provides a set of `DataCollector` classes. They can be used to collect data using our environment and current iteration of the model. For simplicity, we will use `SyncDataCollector`.\n","\n","We will need to set `total_frames` and `frames_per_batch`. Frame is a term that might be more appropriate in video game simulations, but it is effectively an observation that our agent is exposed to. The `total_frames` is a total number of observations that will be collected, and `frames_per_batch` is the amount of frames given to the agent to perform the next set of training updates.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchrl.collectors import SyncDataCollector"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["total_frames = 10000\n","frames_per_batch = 100"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["collector = SyncDataCollector(\n","    transformed_env,\n","    actor,\n","    frames_per_batch=frames_per_batch,\n","    total_frames=total_frames,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# We can test that our collector works with our env and actor like this:\n","collector.rollout()"]},{"cell_type":"markdown","metadata":{},"source":["## Replay buffers\n","\n","TorchRL provides ready implementations of replay buffers, with a choice of storage mechanisms and sampling strategies.\n","\n","For on-policy algorithm such as PPO, we can set the buffer's storage to have the `frames_per_batch` size. This will refill the buffer every time the new batch of data is collected."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchrl.data.replay_buffers import TensorDictReplayBuffer, \\\n","    LazyTensorStorage, SamplerWithoutReplacement"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["#  In on-policy contexts, a replay buffer is refilled every time a batch of data is collected,\n","# and its data is repeatedly consumed for a certain number of epochs.\n","buffer = TensorDictReplayBuffer(\n","    storage=LazyTensorStorage(frames_per_batch),\n","    sampler=SamplerWithoutReplacement()\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Loss function\n","\n","The PPO loss can be directly imported from torchrl for convenience using the `ClipPPOLoss` class.\n","\n","PPO requires some “advantage estimation” to be computed. In short, an advantage is a value that reflects an expectancy over the return value while dealing with the bias / variance tradeoff. To compute the advantage, one just needs to (1) build the advantage module, which utilizes our value operator (critic), and (2) pass each batch of data through it before each epoch. For this advantage function, we will use the GAE module. It will update the input `TensorDict` with new `\"advantage\"` and `\"value_target\"` entries. The `\"value_target\"` is a gradient-free tensor that represents the empirical value that the value network should represent with the input observation. Both of these will be used by `ClipPPOLoss` to return the policy and value losses."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchrl.objectives import ClipPPOLoss\n","from torchrl.objectives.value import GAE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_grad_norm = 1.0\n","\n","# PPO params\n","sub_batch_size = 25  # cardinality of the sub-samples gathered from the current data in the inner loop\n","num_epochs = 10  # optimisation steps per batch of data collected\n","clip_epsilon = (\n","    0.2  # clip value for PPO loss: see the equation in the intro for more context.\n",")\n","gamma = 0.99\n","lmbda = 0.95\n","entropy_eps = 1e-4"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["adv_fn = GAE(\n","    value_network=critic,\n","    gamma=gamma,\n","    lmbda=lmbda,\n","    average_gae=True,\n",")\n","\n","loss_fn = ClipPPOLoss(\n","    actor,\n","    critic,\n","    gamma=gamma,\n","    clip_epsilon=clip_epsilon,\n","    entropy_bonus=bool(entropy_eps),\n","    entropy_coef=entropy_eps,\n","    # these keys match by default but we set this for completeness\n","    value_target_key=adv_fn.value_target_key,\n","    critic_coef=1.0,\n","    loss_critic_type=\"smooth_l1\",\n",")\n","\n","# Now also define optimization algorithm.\n","optim = torch.optim.Adam(loss_fn.parameters(), lr=2e-4)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n","    optim, total_frames // frames_per_batch, 0.0\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Training loop\n","\n","We now have all the pieces needed to code our training loop.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from collections import defaultdict\n","from tqdm.notebook import tqdm\n","\n","from torchrl.envs.utils import ExplorationType, set_exploration_type"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","logs = defaultdict(list)\n","pbar = tqdm(total=total_frames)\n","eval_str = \"\"\n","\n","# We iterate over the collector until it reaches the total number of frames it was\n","# designed to collect:\n","for i, tensordict_data in enumerate(collector):\n","    # we now have a batch of data to work with. Let's learn something from it.\n","    for _ in range(num_epochs):\n","        # We'll need an \"advantage\" signal to make PPO work.\n","        # We re-compute it at each epoch as its value depends on the value\n","        # network which is updated in the inner loop.\n","        with torch.no_grad():\n","            # Updates tensordict in-place with \"advantage\" and \"value_target\" fields.\n","            adv_fn(tensordict_data)\n","\n","        data_view = tensordict_data.reshape(-1)\n","        buffer.extend(data_view.cpu())\n","\n","        for _ in range(frames_per_batch // sub_batch_size):\n","            sample = buffer.sample(sub_batch_size)  # mini-batch\n","            loss_vals = loss_fn(sample)\n","            loss_val = (\n","                loss_vals[\"loss_objective\"]\n","                + loss_vals[\"loss_critic\"]\n","                + loss_vals[\"loss_entropy\"]\n","            )\n","\n","            # Optimization: backward, grad clipping and optim step\n","            loss_val.backward()\n","            # this is not strictly mandatory but it's good practice to keep\n","            # your gradient norm bounded\n","            torch.nn.utils.clip_grad_norm_(loss_fn.parameters(), max_grad_norm)\n","            optim.step()\n","            optim.zero_grad()\n","\n","    print(f\"avg reward: {tensordict_data['next', 'reward'].mean().item(): 4.4f}\")\n","    logs[\"reward\"].append(tensordict_data[\"next\", \"reward\"].mean().item())\n","    pbar.update(tensordict_data.numel())\n","    cum_reward_str = (\n","        f\"average reward={logs['reward'][-1]: 4.4f} (init={logs['reward'][0]: 4.4f})\"\n","    )\n","    # logs[\"step_count\"].append(tensordict_data[\"step_count\"].max().item())\n","    # stepcount_str = f\"step count (max): {logs['step_count'][-1]}\"\n","    logs[\"lr\"].append(optim.param_groups[0][\"lr\"])\n","    lr_str = f\"lr policy: {logs['lr'][-1]: 4.4f}\"\n","    if i % 10 == 0:\n","        # We evaluate the policy once every 10 batches of data.\n","        # Evaluation is rather simple: execute the policy without exploration\n","        # (take the expected value of the action distribution) for a given\n","        # number of steps (1000, which is our env horizon).\n","        # The ``rollout`` method of the env can take a policy as argument:\n","        # it will then execute this policy at each step.\n","        with set_exploration_type(ExplorationType.MEAN), torch.no_grad():\n","            # execute a rollout with the trained policy\n","            eval_rollout = env.rollout(500, actor)\n","            logs[\"eval reward\"].append(eval_rollout[\"next\", \"reward\"].mean().item())\n","            logs[\"eval reward (sum)\"].append(\n","                eval_rollout[\"next\", \"reward\"].sum().item()\n","            )\n","            # logs[\"eval step_count\"].append(eval_rollout[\"step_count\"].max().item())\n","            eval_str = (\n","                f\"eval cumulative reward: {logs['eval reward (sum)'][-1]: 4.4f} \"\n","                f\"(init: {logs['eval reward (sum)'][0]: 4.4f}), \"\n","                # f\"eval step-count: {logs['eval step_count'][-1]}\"\n","            )\n","            del eval_rollout\n","    pbar.set_description(\", \".join([eval_str, cum_reward_str, lr_str]))\n","\n","    # We're also using a learning rate scheduler. Like the gradient clipping,\n","    # this is a nice-to-have but nothing necessary for PPO to work.\n","    scheduler.step()\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Acknowledgements\n","\n","* This guide is heavily based on TorchRL's PPO guide: https://pytorch.org/rl/tutorials/coding_ppo.html\n","* Maxyme Szimanski's notebook: https://www.kaggle.com/code/maximeszymanski/ppo-deep-reinforcement-learning"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":7339171,"sourceId":65704,"sourceType":"competition"}],"dockerImageVersionId":30635,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"torchrl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":4}
