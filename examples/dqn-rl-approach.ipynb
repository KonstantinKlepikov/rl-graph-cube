{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":65704,"databundleVersionId":7339171,"sourceType":"competition"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\nThis notebook takes an RL approach using Deep Q Networks to solve the challenge. <br>\nSo far I have not had much luck, only occasionally reaching better solutions for wreaths, but no solutions for any other problems. <br>\n\nI am relatively new to RL and wanted to explore it via this challenge, so any feedback and comments are very appreciated. <br>\nThis is also my first public Kaggle Notebook, so any advice on structure/conventions in Kaggle are also welcome!","metadata":{}},{"cell_type":"markdown","source":"# Hyperparameters and Constants","metadata":{}},{"cell_type":"code","source":"puzzle_info_path = '/kaggle/input/santa-2023/puzzle_info.csv'\npuzzles_path = '/kaggle/input/santa-2023/puzzles.csv'\nsample_submission_path = '/kaggle/input/santa-2023/sample_submission.csv'","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:28.450312Z","iopub.execute_input":"2024-01-07T16:01:28.450784Z","iopub.status.idle":"2024-01-07T16:01:28.457165Z","shell.execute_reply.started":"2024-01-07T16:01:28.450745Z","shell.execute_reply":"2024-01-07T16:01:28.455593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\nNUM_EPISODES = 1\nMAX_STEPS = 1_000\nCHECKPOINT_AT = 25\n\nBATCH_SIZE = 32\nGAMMA = 0.995\nGAMMA_LARGE = 0.9995\nEPS_START = 0.990\nEPS_END = 0.001\nEPS_DECAY = 1000\nTAU = 0.1\nLR = 1e-4\n\nTRAIN = True\nCONTINUED_RUN = False\nBASE_DIR = os.getcwd()\n\nEXP_PATH = os.path.join(BASE_DIR, 'experiments')","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:28.466265Z","iopub.execute_input":"2024-01-07T16:01:28.466779Z","iopub.status.idle":"2024-01-07T16:01:28.474554Z","shell.execute_reply.started":"2024-01-07T16:01:28.466738Z","shell.execute_reply":"2024-01-07T16:01:28.473083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utilities","metadata":{}},{"cell_type":"code","source":"import re\n\nimport math\nimport random\n\nfrom collections import namedtuple, deque\n\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:28.477616Z","iopub.execute_input":"2024-01-07T16:01:28.478143Z","iopub.status.idle":"2024-01-07T16:01:28.991701Z","shell.execute_reply.started":"2024-01-07T16:01:28.4781Z","shell.execute_reply":"2024-01-07T16:01:28.989766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pz_info = pd.read_csv(puzzle_info_path)\npuzzles = pd.read_csv(puzzles_path)\nss = pd.read_csv(sample_submission_path)\n\nss['puzzle_type'] = puzzles['puzzle_type']\n\npuzzles['initial_state'] = puzzles['initial_state'].apply(lambda x: x.split(';'))\npuzzles['solution_state'] = puzzles['solution_state'].apply(lambda x: x.split(';'))\npuzzles['moves_in_ss'] = ss['moves'].apply(lambda x: x.split('.')).apply(len)\n\nlegal_moves = {}\nfor puzzle_type in pz_info['puzzle_type'].unique():\n    moves = eval(pz_info[pz_info['puzzle_type'] == puzzle_type]['allowed_moves'].values[0])\n    moves_ = moves.copy()\n    for m, d in moves_.items():\n        moves[f'-{m}'] = list(np.argsort(d))\n    legal_moves[puzzle_type] = moves\n\nall_moves = set()\nfor puzzle_type in legal_moves.keys():\n    for k, v in legal_moves[puzzle_type].items():\n        all_moves.add(k)\nall_moves = sorted(list(all_moves))\n\n# ACTION_SPACE_SIZE = 268\n# len(all_moves)\n\n# OBS_SPACE_SIZE = 6534\n# max(len(list(legal_moves[puzzle_type].values())[0]) for puzzle_type in legal_moves.keys())\n\nNUM_PUZZLES = 398\n# puzzles.shape[0]","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:28.99522Z","iopub.execute_input":"2024-01-07T16:01:28.996079Z","iopub.status.idle":"2024-01-07T16:01:31.663399Z","shell.execute_reply.started":"2024-01-07T16:01:28.996025Z","shell.execute_reply":"2024-01-07T16:01:31.66221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n\nclass ReplayMemory(object):\n\n    def __init__(self, capacity):\n        self.memory = deque([], maxlen=capacity)\n\n    def push(self, *args):\n        \"\"\"Save a transition\"\"\"\n        self.memory.append(Transition(*args))\n\n    def sample(self, batch_size):\n        return random.sample(self.memory, batch_size)\n\n    def __len__(self):\n        return len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:31.665309Z","iopub.execute_input":"2024-01-07T16:01:31.666235Z","iopub.status.idle":"2024-01-07T16:01:31.675982Z","shell.execute_reply.started":"2024-01-07T16:01:31.666186Z","shell.execute_reply":"2024-01-07T16:01:31.675025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def epsilon_decay(step, max_step):\n    return EPS_END + (EPS_START - EPS_END) * math.exp(-1. * step / max_step)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:31.679115Z","iopub.execute_input":"2024-01-07T16:01:31.679796Z","iopub.status.idle":"2024-01-07T16:01:31.688611Z","shell.execute_reply.started":"2024-01-07T16:01:31.679733Z","shell.execute_reply":"2024-01-07T16:01:31.687288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dividing the puzzles\n\nI am dividing the problems into 5 categories. These categories will be trained and tested in 5 different environments. <br>\nThe 5 categories are: small cubes, large cubes, small globes, large globes, and wreaths","metadata":{}},{"cell_type":"code","source":"def divide_puzzles(puzzles):\n    small_cubes = ['cube_2/2/2', 'cube_3/3/3', 'cube_4/4/4', 'cube_5/5/5']\n    small_cubes_condition = puzzles['puzzle_type'].str.contains(small_cubes[0])\n    for sc in small_cubes[1:]:\n        small_cubes_condition |= puzzles['puzzle_type'].str.contains(sc)\n    large_cubes_condition = puzzles['puzzle_type'].str.contains('cube') & ~small_cubes_condition\n    \n    small_globes = ['globe_1/8', 'globe_2/6', 'globe_3/4','globe_1/16']\n    small_globes_condition = puzzles['puzzle_type'].str.contains(small_globes[0])\n    for sg in small_globes[1:]:\n        small_globes_condition |= puzzles['puzzle_type'].str.contains(sg)\n    large_globes_condition = puzzles['puzzle_type'].str.contains('globe') & ~small_globes_condition\n    \n    wreaths_condition = puzzles['puzzle_type'].str.contains('wreath')\n    \n    return [\n        puzzles[small_cubes_condition],\n        puzzles[large_cubes_condition],\n        puzzles[small_globes_condition],\n        puzzles[large_globes_condition],\n        puzzles[wreaths_condition]\n    ]\n\ndef combine_solutions(*sols):\n    return pd.concat(*sols).sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:31.690778Z","iopub.execute_input":"2024-01-07T16:01:31.691649Z","iopub.status.idle":"2024-01-07T16:01:31.702006Z","shell.execute_reply.started":"2024-01-07T16:01:31.691607Z","shell.execute_reply":"2024-01-07T16:01:31.700928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment","metadata":{}},{"cell_type":"code","source":"import string\nimport logging\n\nfrom collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nimport gymnasium as gym\nfrom gymnasium import spaces","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:31.703585Z","iopub.execute_input":"2024-01-07T16:01:31.704591Z","iopub.status.idle":"2024-01-07T16:01:32.015755Z","shell.execute_reply.started":"2024-01-07T16:01:31.704548Z","shell.execute_reply":"2024-01-07T16:01:32.014621Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reward function","metadata":{}},{"cell_type":"code","source":"def reward_function(curr_state, prev_state, solution, num_wildcards, moves_played, moves_in_ss, solving_penalty=0.005):\n    mult = (100 * moves_in_ss) / (len(solution) * moves_played)\n    curr_r = np.sum(curr_state == solution)\n    if curr_r >= len(solution) - num_wildcards:\n        return np.round(curr_r * mult, 6)\n    prev_r = np.sum(prev_state == solution)\n    return np.round(max(curr_r, prev_r) * mult * solving_penalty, 6)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:32.017522Z","iopub.execute_input":"2024-01-07T16:01:32.01795Z","iopub.status.idle":"2024-01-07T16:01:32.025651Z","shell.execute_reply.started":"2024-01-07T16:01:32.017904Z","shell.execute_reply":"2024-01-07T16:01:32.024286Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def apply_move(state, move, puzzle_type):\n    return state[legal_moves[puzzle_type][move]]\n\ndef is_solution(state, solution, num_wc):\n    return True if np.sum(state != solution) - num_wc <= 0 else False\n\ndef rNone():\n    return None","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:32.02762Z","iopub.execute_input":"2024-01-07T16:01:32.028221Z","iopub.status.idle":"2024-01-07T16:01:32.037417Z","shell.execute_reply.started":"2024-01-07T16:01:32.028168Z","shell.execute_reply":"2024-01-07T16:01:32.036061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SantaEnv(gym.Env):   \n    def __init__(self,\n                 puzzles,\n                 env_name = None,\n                 render_mode=None):\n        super().__init__()\n        self.render_mode = render_mode\n        self.alphabet = {k:i for i, k in enumerate(string.ascii_letters)}\n        \n        self.legal_moveset = []\n        for pzt in puzzles.puzzle_type.unique():\n            self.legal_moveset.extend(list(legal_moves[pzt].keys()))\n        self.legal_moveset = list(set(self.legal_moveset))\n        \n        self.action_to_move = {i:k for i, k in enumerate(self.legal_moveset)}\n        \n        self.testing = False\n        \n        self.puzzles = puzzles\n        self.env_name = env_name if env_name is not None else 'not_named'\n                \n        self.state_size = max(puzzles.initial_state.apply(len))\n        self.action_size = len(self.legal_moveset)\n        self.observation_space = spaces.Box(0, self.state_size-1, shape=(self.state_size,), dtype=np.int64)\n        self.action_space = spaces.Discrete(self.action_size)\n        \n        self.reward_range = (0, self.state_size*10+1)\n        self.discount = 1\n        \n        self.solution_dict = defaultdict(rNone)\n        # {-1: {\"id\": -1, \"moves\": \"f1.f0\", \"puzzle_type\":\"placeholder\"}}\n        \n        self.step_counter = 0\n        self.solution_state = None\n        self.current_state = None\n        self.prev_state = None\n        self.moves_in_ss = 0\n        \n        self.num_wc = 0\n        self.current_puzzle = None\n        self.current_puzzle_id = None\n        self.current_puzzle_type = None\n        \n        self.valid_moves = None\n        self.actions_taken = []\n        \n        self.max_step = MAX_STEPS\n    \n    def state_to_numeric(self, state):\n        if state is None:\n            return None\n        obs_mask = np.ones((self.state_size,)) * -1\n        prefix = np.array([self.alphabet[k] if k in string.ascii_letters else int(k[1:])+len(string.ascii_letters) for k in state])\n        obs_mask[:len(prefix)] = prefix\n        return obs_mask\n    \n    def _get_obs(self):\n        return self.state_to_numeric(self.current_state)\n    \n    def _get_info(self):\n        return {}\n    \n    def reset(self, index=None, seed=None):\n        super().reset(seed=seed)\n        \n        if index is None:\n            self.current_puzzle = self.puzzles.iloc[np.random.randint(0, self.puzzles.shape[0]-1)]\n        else:\n            self.current_puzzle = self.puzzles.loc[index]\n        \n        self.current_puzzle_id = self.current_puzzle.id\n        if not self.testing:\n            print(f\"<{self.env_name}> Puzzle ID: {self.current_puzzle_id}\")\n\n        self.current_puzzle_type = self.current_puzzle.puzzle_type\n        self.current_state = np.array(self.current_puzzle.initial_state)\n        self.solution_state = np.array(self.current_puzzle.solution_state)\n\n        self.num_wc = self.current_puzzle.num_wildcards\n        self.valid_moves = list(legal_moves[self.current_puzzle_type].keys())\n        self.moves_in_ss = self.current_puzzle.moves_in_ss\n        self.actions_taken = []\n                \n        self.step_counter = 0\n        self.max_step = max(MAX_STEPS, int(self.moves_in_ss * 0.75))\n        self.done = False\n        self.discount = 1\n        \n        obs = self._get_obs()\n        info = self._get_info()\n        \n        return obs, info\n    \n    def step(self, action, discount = GAMMA):\n        self.prev_state = self.current_state.copy()\n        self.current_state = apply_move(self.current_state,\n                                            self.action_to_move[action],\n                                            self.current_puzzle_type)        \n        self.step_counter += 1\n        self.actions_taken.append(action)\n        \n        terminated = False\n        truncated = False\n        if is_solution(self.current_state, self.solution_state, self.num_wc):\n            if not self.testing:\n                print(f\"<{self.env_name}> {self.current_puzzle_id} solved!\")\n            \n            moves = \".\".join([self.action_to_move[a] for a in self.actions_taken])\n            if self.solution_dict[self.current_puzzle_id] is None or \\\n                len(self.solution_dict[self.current_puzzle_id]['moves'].split('.')) > len(moves.split('.')):\n                self.solution_dict[self.current_puzzle_id] = {\"id\": self.current_puzzle_id,\n                                                              \"moves\": moves,\n                                                              \"puzzle_type\": self.current_puzzle_type}\n            terminated = True\n\n        if self.step_counter > self.max_step and not terminated:\n            if not self.testing:\n                print(f\"<{self.env_name}> Max Steps Reached!\")\n            truncated = True\n        \n        reward = self.discount * reward_function(self.current_state, self.prev_state, self.solution_state, self.num_wc, self.step_counter+1, self.moves_in_ss)\n        self.discount *= discount\n        obs = self._get_obs()\n        info = self._get_info()\n        \n        return obs, np.round(reward, 6), terminated, truncated, info\n    \n    def close(self):\n        super().close()\n    \n    def action_mask(self):\n        return np.array([1 if k in self.valid_moves else 0 for k in self.legal_moveset])\n    \n    def get_random_valid_action(self):\n        return np.random.choice(np.argwhere(self.action_mask() == 1).squeeze())\n    \n    def get_computed_moves(self):\n        moves = pd.DataFrame.from_dict(self.solution_dict, orient='index')\n        return pd.concat([moves, ss.iloc[self.puzzles.index]]).drop_duplicates('id', keep='first').sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:32.040075Z","iopub.execute_input":"2024-01-07T16:01:32.041343Z","iopub.status.idle":"2024-01-07T16:01:32.076257Z","shell.execute_reply.started":"2024-01-07T16:01:32.041295Z","shell.execute_reply":"2024-01-07T16:01:32.075072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# DQN and DQNAgent","metadata":{}},{"cell_type":"code","source":"import math\nimport random\nimport logging\n\nfrom itertools import count\n\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:32.081449Z","iopub.execute_input":"2024-01-07T16:01:32.082348Z","iopub.status.idle":"2024-01-07T16:01:34.056379Z","shell.execute_reply.started":"2024-01-07T16:01:32.082297Z","shell.execute_reply":"2024-01-07T16:01:34.054846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:34.058133Z","iopub.execute_input":"2024-01-07T16:01:34.058827Z","iopub.status.idle":"2024-01-07T16:01:34.065285Z","shell.execute_reply.started":"2024-01-07T16:01:34.058785Z","shell.execute_reply":"2024-01-07T16:01:34.063827Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DQN(nn.Module):\n\n    def __init__(self, n_observations, n_actions):\n        super(DQN, self).__init__()\n                \n        self.layer1 = nn.Linear(n_observations, 256)\n        self.layer2 = nn.Linear(256, 128)\n        self.layer3 = nn.Linear(128, n_actions)\n\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        return self.layer3(x)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:34.067143Z","iopub.execute_input":"2024-01-07T16:01:34.067859Z","iopub.status.idle":"2024-01-07T16:01:34.077439Z","shell.execute_reply.started":"2024-01-07T16:01:34.067816Z","shell.execute_reply":"2024-01-07T16:01:34.076132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DQNAgent(nn.Module):\n    def __init__(self,\n                 env,\n                 criterion = None):\n        super().__init__()\n        \n        self.env = env\n        self.state_size = env.state_size\n        self.action_size = env.action_size\n\n        self.criterion = nn.SmoothL1Loss() if criterion is None else criterion()\n        \n        self.memory = ReplayMemory(1000)\n        \n        self.policy_net = DQN(self.state_size, self.action_size).to(device)\n        self.target_net = DQN(self.state_size, self.action_size).to(device)\n        \n        self.target_net.load_state_dict(self.policy_net.state_dict())\n        \n        self.target_net.eval()\n        self.optimizer = torch.optim.AdamW(self.policy_net.parameters(), lr=LR, amsgrad=True)\n        \n        self.gamma = GAMMA_LARGE if 'large' in self.env.env_name else GAMMA\n            \n    def select_action(self, state, testing=False):\n        global device\n        if testing or random.random() > epsilon_decay(self.env.step_counter, self.env.max_step):\n            with torch.no_grad():\n                a = self.policy_net(state).squeeze() * torch.tensor(self.env.action_mask()).to(device)\n                a = a.masked_fill(a == 0, -torch.inf)\n                return a.argmax().view(1, 1)\n        else:\n            return torch.tensor([[self.env.get_random_valid_action()]], device=device, dtype=torch.long)\n        \n    def isNone(self, s):\n        return s is not None\n    \n    def optimize_agent(self):\n        global device\n        \n        if len(self.memory) < BATCH_SIZE:\n            return\n        \n        transitions = self.memory.sample(BATCH_SIZE)\n        batch = Transition(*zip(*transitions))\n        \n        non_final_mask = torch.tensor(tuple(map(self.isNone, batch.next_state)), device=device, dtype=torch.bool)\n        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n        \n        state_batch = torch.cat(batch.state)\n        action_batch = torch.cat(batch.action)\n        reward_batch = torch.cat(batch.reward)\n        \n        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n        \n        next_state_values = torch.zeros(BATCH_SIZE, device=device)\n        \n        with torch.no_grad():\n            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1).values\n        expected_state_action_values = (next_state_values) + reward_batch\n        \n        loss = self.criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n\n        self.optimizer.zero_grad()\n        loss.backward()\n        # In-place gradient clipping\n        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n        self.optimizer.step()\n    \n    def train_agent(self, num_episodes=NUM_EPISODES):\n        global device\n        \n        self.rewards_per_ep = []\n        for i_ep in range(num_episodes):\n            print(f\"<{self.env.env_name}> Episode {i_ep}\")\n            \n            state, _ = self.env.reset()\n            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n            ep_reward = 0\n            \n            for t in count():\n                action = self.select_action(state)\n                observation, reward, terminated, truncated, _ = self.env.step(action.item(), self.gamma)\n                ep_reward += reward\n                reward = torch.tensor([reward], device=device)\n                done = terminated or truncated\n                \n                if terminated:\n                    next_state = None\n                    print(f\"<{self.env.env_name}> {self.env.solution_dict.keys()}\")\n                else:\n                    next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n                \n                self.memory.push(state, action, next_state, reward)\n                state = next_state\n                \n                self.optimize_agent()\n                \n                target_net_state_dict = self.target_net.state_dict()\n                policy_net_state_dict = self.policy_net.state_dict()\n                for key in policy_net_state_dict:\n                    target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n                self.target_net.load_state_dict(target_net_state_dict)\n                \n                \n                if done:\n                    self.rewards_per_ep.append(ep_reward)\n                    print(f\"<{self.env.env_name}> Episode {i_ep} ID: {self.env.current_puzzle_id} Mean Reward per Step: {ep_reward/t}\")\n                    break\n    \n    def test_agent(self):\n        self.env.testing = True\n        \n        self.rewards_per_pz = []\n        for pz in list(self.env.puzzles.index):\n            state, _ = self.env.reset(pz)\n            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n            \n            ep_reward = 0\n            while True:\n                action = self.select_action(state, True)\n                observation, reward, terminated, truncated, _ = self.env.step(action.item(), self.gamma)\n                ep_reward += reward\n                \n                if terminated:\n                    print(f\"<{self.env.env_name}> Puzzle {pz} solved! Mean Reward per Step: {ep_reward/self.env.step_counter}\")\n                    break\n                elif truncated:\n                    print(f\"<{self.env.env_name}> Puzzle {pz} not solved!\")\n                    break\n            \n            self.rewards_per_pz.append(ep_reward)\n        self.env.testing = False","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:02:46.172056Z","iopub.execute_input":"2024-01-07T16:02:46.172719Z","iopub.status.idle":"2024-01-07T16:02:46.208529Z","shell.execute_reply.started":"2024-01-07T16:02:46.172635Z","shell.execute_reply":"2024-01-07T16:02:46.207163Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:01:34.132169Z","iopub.execute_input":"2024-01-07T16:01:34.133232Z","iopub.status.idle":"2024-01-07T16:01:34.140433Z","shell.execute_reply.started":"2024-01-07T16:01:34.133093Z","shell.execute_reply":"2024-01-07T16:01:34.138978Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"envs = [SantaEnv(pzs, name) for pzs, name in zip(divide_puzzles(puzzles), \\\n            ['small_cubes', 'large_cubes', 'small_globes', 'large_globes', 'wreaths'])]\ndqns = [DQNAgent(env).to(device) for env in envs]","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:02:49.18967Z","iopub.execute_input":"2024-01-07T16:02:49.190836Z","iopub.status.idle":"2024-01-07T16:02:49.262054Z","shell.execute_reply.started":"2024-01-07T16:02:49.190789Z","shell.execute_reply":"2024-01-07T16:02:49.261016Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if TRAIN:\n    for agent in dqns:\n        agent.train_agent()\nelse:\n    print(\"Skipping training...\")","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:02:51.839785Z","iopub.execute_input":"2024-01-07T16:02:51.840483Z","iopub.status.idle":"2024-01-07T16:07:02.380873Z","shell.execute_reply.started":"2024-01-07T16:02:51.840445Z","shell.execute_reply":"2024-01-07T16:07:02.379407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for agent in dqns:\n    agent.test_agent()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T16:07:02.383488Z","iopub.execute_input":"2024-01-07T16:07:02.384713Z","iopub.status.idle":"2024-01-07T17:06:19.537545Z","shell.execute_reply.started":"2024-01-07T16:07:02.384639Z","shell.execute_reply":"2024-01-07T17:06:19.536291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"moves = combine_solutions([env.get_computed_moves() for env in envs]) ","metadata":{"execution":{"iopub.status.busy":"2024-01-07T17:06:19.539789Z","iopub.execute_input":"2024-01-07T17:06:19.540426Z","iopub.status.idle":"2024-01-07T17:06:19.569015Z","shell.execute_reply.started":"2024-01-07T17:06:19.540389Z","shell.execute_reply":"2024-01-07T17:06:19.567737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ss['move_length'] = ss['moves'].apply(lambda x: x.split('.')).apply(len)\nmoves['move_length'] = moves['moves'].apply(lambda x: x.split('.')).apply(len)\n\nbool_mask = ss['move_length'] > moves['move_length']\nsubmission = pd.concat([moves[bool_mask], ss]).drop_duplicates('id', keep='first').sort_index()","metadata":{"execution":{"iopub.status.busy":"2024-01-07T17:06:19.572268Z","iopub.execute_input":"2024-01-07T17:06:19.574046Z","iopub.status.idle":"2024-01-07T17:06:19.929038Z","shell.execute_reply.started":"2024-01-07T17:06:19.573982Z","shell.execute_reply":"2024-01-07T17:06:19.92758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.drop(['puzzle_type', 'move_length'], axis=1, inplace=True)\nsubmission.to_csv('./submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-01-07T17:06:19.931122Z","iopub.execute_input":"2024-01-07T17:06:19.931508Z","iopub.status.idle":"2024-01-07T17:06:20.195347Z","shell.execute_reply.started":"2024-01-07T17:06:19.931475Z","shell.execute_reply":"2024-01-07T17:06:20.193966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# References\n\n1. https://www.kaggle.com/code/maximeszymanski/ppo-deep-reinforcement-learning/notebook\n2. https://www.kaggle.com/code/squarehare/q-learning-reinforcement-learning\n3. https://www.kaggle.com/code/robikscube/santa-2023-polytope-permutation-first-look\n4. https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n5. https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/","metadata":{}}]}